Sender: LSF System <lsfadmin@eu-a6-004-11>
Subject: Job 83039846: <sh R --vanilla --slave -f ../../DetectShifts_t4_MGPM_A_F_all.R --args 19> in cluster <euler> Exited

Job <sh R --vanilla --slave -f ../../DetectShifts_t4_MGPM_A_F_all.R --args 19> was submitted from host <eu-login-01-ng> by user <vmitov> in cluster <euler> at Sat Jan 12 20:57:41 2019
Job was executed on host(s) <5*eu-a6-004-11>, in queue <normal.24h>, as user <vmitov> in cluster <euler> at Sat Jan 12 21:24:42 2019
                            <5*eu-a6-006-01>
                            <23*eu-a6-008-02>
                            <4*eu-a6-009-13>
                            <15*eu-a6-006-04>
                            <14*eu-a6-008-16>
                            <4*eu-a6-004-09>
                            <7*eu-a6-001-17>
                            <8*eu-a6-006-13>
                            <4*eu-a6-008-10>
                            <7*eu-a6-008-23>
                            <4*eu-a6-008-01>
</cluster/home/vmitov> was used as the home directory.
</cluster/home/vmitov/MGPMSimulations/data-raw/Results_t4/MGPM_A_F_all_19> was used as the working directory.
Started at Sat Jan 12 21:24:42 2019
Terminated at Sat Jan 12 22:35:39 2019
Results reported at Sat Jan 12 22:35:39 2019

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
sh R --vanilla --slave -f ../../DetectShifts_t4_MGPM_A_F_all.R --args 19
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   419007.00 sec.
    Max Memory :                                 25607 MB
    Average Memory :                             22962.43 MB
    Total Requested Memory :                     102400.00 MB
    Delta Memory :                               76793.00 MB
    Max Swap :                                   -
    Max Processes :                              114
    Max Threads :                                315
    Run time :                                   4268 sec.
    Turnaround time :                            5878 sec.

The output (if any) follows:

Loading required package: Rcpp
Loading required package: doMPI
Loading required package: foreach
Loading required package: iterators
Loading required package: Rmpi
Master processor name: eu-a6-004-11; nodename: eu-a6-004-11
Size of MPI universe: 1
Spawning 99 workers using the command:
  /cluster/apps/r/3.4.0_openblas/x86_64/lib64/R/bin/Rscript /cluster/apps/r/3.4.0_openblas/x86_64/lib64/R/library/doMPI/RMPIworker.R WORKDIR=/cluster/home/vmitov/MGPMSimulations/data-raw/Results_t4/MGPM_A_F_all_19 LOGDIR=/cluster/home/vmitov/MGPMSimulations/data-raw/Results_t4/MGPM_A_F_all_19 MAXCORES=1 COMM=3 INTERCOMM=4 MTAG=10 WTAG=11 INCLUDEMASTER=TRUE BCAST=TRUE VERBOSE=TRUE
	99 slaves are spawned successfully. 0 failed.
--------------------------------------------------------------------------
An MPI process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your MPI job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.  

The process that invoked fork was:

  Local host:          eu-a6-008-23 (PID 49281)
  MPI_COMM_WORLD rank: 94

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
$PCMBase.Value.NA
[1] -1e+20

$PCMBase.Errors.As.Warnings
[1] TRUE

$PCMBase.Threshold.Lambda_ij
[1] 1e-08

$PCMBase.Threshold.EV
[1] 1e-07

$PCMBase.Threshold.SV
[1] 1e-06

$PCMBase.Threshold.Skip.Singular
[1] 1e-04

$PCMBase.Skip.Singular
[1] TRUE

$PCMBase.Tolerance.Symmetric
[1] 1e-08

$PCMBase.Lmr.mode
[1] 11

$PCMBase.ParamValue.LowerLimit.NonNegativeDiagonal
[1] 0

$PCMBase.ParamValue.LowerLimit
[1] -10

$PCMBase.ParamValue.UpperLimit
[1] 10

Initiating tableFits...
Step 1: Performing fits on 6  clades;  36  model mappings altogether...
[eu-a6-004-11:69091] 98 more processes have sent help message help-mpi-runtime.txt / mpi_init:warn-fork
[eu-a6-004-11:69091] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Step 2: Perform a recursive clade partition search of an optimal mixed model...
Step 2.1: Initiated queue of partition root nodes with the root of the original tree...
Step 2.2: headQPR= 1 : bestPartition / mapping / score: ( 81 ) / ( F ) /  299.7782  ; 
  Remaining queue:
Empty data.table (0 rows) of 5 cols: level,node,partitionParentNode,partInit,partCurrent
Performing clade-partitioning at partitionRootLabel= 81 ; partitionRootLevel= 1 
Step 2.3: numTips in subtree =  80 
Step 2.5: List of partitions of the tree:
81
81, 129
81, 128
81, 122
81, 121
81, 121, 129
81, 101
81, 101, 129
81, 101, 128
81, 101, 122
Step 2.6: Preparing allowed model-types for each clade-partition
listAllowedModelTypes:
 `81`: A, B, C, D, E, F
`129`: A, B, C, D, E, F
`128`: A, B, C, D, E, F
`122`: A, B, C, D, E, F
`121`: A, B, C, D, E, F
`101`: A, B, C, D, E, F
Error in checkCluster(cl) : not a valid cluster
Calls: <Anonymous> -> defaultCluster -> checkCluster
Execution halted
